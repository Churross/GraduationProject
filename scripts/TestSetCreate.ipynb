{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test data set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import editdistance\n",
    "from tqdm import tqdm\n",
    "from Bio import AlignIO\n",
    "import scipy.cluster.hierarchy as shc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_suffix = '_test'\n",
    "output_path = \"output/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ../scripts/output/stays-test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_pickle(data, file_name, path=output_path):\n",
    "    file = open(path + file_name, 'wb')\n",
    "    pickle.dump(data, file)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "def get_pickle(file_name, path=output_path):\n",
    "    return pickle.load(open(path + file_name, 'ab'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test set from sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seqs_1 = [\"aaaaaaaaab\", \"aaaaaaaab\", \"aaaaaaab\", \"aaaaaab\", \"aaaaab\"]\n",
    "seqs_1 = [\"aaaaaaaaa\",\n",
    "          \"aaaaaaaya\",\n",
    "           \"abb\",\n",
    "           \"bbb\",\n",
    "           \"aba\",\n",
    "           \"abaa\",\n",
    "           \"dbbddd\",\n",
    "           \"dbbdd\",\n",
    "           \"eedbbdd\",\n",
    "           \"dbbddgh\",\n",
    "           \"ccccccccc\",    \n",
    "           \"cccccccgc\",    \n",
    "          ]\n",
    "# seqs_1 = [\n",
    "#             \"bbb\",\n",
    "#            \"aba\",\n",
    "#            \"abaa\",\n",
    "#            \"abbaa\",\n",
    "#           ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame()\n",
    "date_time = datetime.datetime(2022, 1, 30, 18, 31)\n",
    "\n",
    "for idx, seq in enumerate(seqs_1):\n",
    "    data_seq = pd.DataFrame([i for i in seq], columns=['eventtype'])\n",
    "    data_seq['subject_id'] = 10201 + idx\n",
    "    data_seq['hadm_id'] = 20201 + idx\n",
    "    data_seq['transfer_id'] = [i + 100 * idx for i in range(len(seq))]\n",
    "    data_seq['careunit'] = 'Emergency Department'\n",
    "    data_seq['intime'] = [date_time + datetime.timedelta(0,i) for i in range(len(seq))]\n",
    "    data_seq['outtime'] = data_seq['intime']\n",
    "    data_seq['charttime'] = data_seq['intime']\n",
    "    # data_seq['intime'] = datetime.datetime(2022, 1, 30, 18, 31)\n",
    "    # data_seq['outtime'] = datetime.datetime(2022, 1, 30, 18, 31)\n",
    "    # data_seq['charttime'] = datetime.datetime(2022, 1, 30, 18, 31)\n",
    "    data_seq = data_seq.set_index('transfer_id')\n",
    "    test_data = pd.concat([test_data, data_seq])\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['event'] = 'transfer: ' + test_data['eventtype']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_types = pd.DataFrame(columns=['subject_id',\t'hadm_id',\t'transfer_id',\t'eventtype',\t'careunit',\t'intime',\t'outtime',\n",
    "                              'charttime',\t'event',\t'value',\t'valuenum',\t'valueuom',\t'label',\t'category',\t'param_type',\t'value_categorical'])\n",
    "all_data_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_data.merge(all_data_types, how='left')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['event_encoded'] = data['eventtype']\n",
    "len(data.event_encoded.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(by=['charttime'])\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.insert(0, 'event_id', range(0, 0 + len(data)))\n",
    "data.set_index('event_id')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_pickle(data, 'data_test')\n",
    "data.to_csv(\"output/data_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_data = data[['event_id', 'hadm_id', 'event_encoded']]\n",
    "save_as_pickle(distance_data, 'distance_test_data')\n",
    "distance_data.to_csv('output/distance_test_data.csv')\n",
    "distance_data.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "number_of_stays = 'test'\n",
    "dist_data = distance_data\n",
    "output_path = \"output/\"\n",
    "output_folder = f\"stays-test/\"\n",
    "alignments_output = f\"{output_folder}alignments/\"\n",
    "file_suffix = '_test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stays = list(dist_data['hadm_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_pickle(data, 'events' + file_suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix():\n",
    "    sequences = [dist_data[dist_data['hadm_id']\n",
    "                           == hadm_id]['event_encoded'].tolist() for hadm_id in stays]\n",
    "\n",
    "    print(\"[INFO] Data Loaded \")\n",
    "\n",
    "    length = len(sequences)\n",
    "    outputMatrix = [[0] * length for _i in range(length)]\n",
    "\n",
    "    progress = 0\n",
    "    updateStep = 100\n",
    "    with tqdm(total=0.5*(length * length)) as pbar:\n",
    "        for idxA in range(0, length):\n",
    "            for idxB in range(idxA, length):\n",
    "                max_length = max(len(sequences[idxA]), len(sequences[idxB]))\n",
    "                distance = editdistance.eval(\n",
    "                    sequences[idxA], sequences[idxB])/max_length\n",
    "                outputMatrix[idxA][idxB] = distance\n",
    "                outputMatrix[idxB][idxA] = distance\n",
    "                if (progress % updateStep == 0):\n",
    "                    pbar.update(updateStep)\n",
    "                progress += 1\n",
    "\n",
    "    return outputMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = compute_distance_matrix()\n",
    "save_as_pickle(dist_matrix, 'distance_matrix_' + str(number_of_stays))\n",
    "save_as_pickle(dist_matrix, 'dist_matrix_' + str(number_of_stays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(editdistance.eval('bbb', 'aba')/3)\n",
    "# print(editdistance.eval('bbb', 'abaa')/4)\n",
    "# print(editdistance.eval('bbb', 'abbaa')/5)\n",
    "# print(editdistance.eval('aba', 'abaa')/4)\n",
    "# print(editdistance.eval('aba', 'abbaa')/5)\n",
    "print(editdistance.eval('abaa', 'abbaa')/5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dist_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_distance_matrix(u, v):\n",
    "    index_u, index_v = stays.index(u[0]), stays.index(v[0])\n",
    "    return dist_matrix[index_u][index_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_data = data.drop_duplicates(subset=['hadm_id'])\n",
    "\n",
    "clust_data = clust_data.drop(columns=['event_id', 'subject_id', 'transfer_id', 'eventtype',\n",
    "                                      'careunit', 'intime', 'outtime', 'charttime', 'event',\n",
    "                                      'value', 'valuenum', 'valueuom',\n",
    "                                      'label', 'category', 'param_type',\n",
    "                                      'value_categorical',\n",
    "                                      'event_encoded'])\n",
    "\n",
    "# links = shc.linkage(clust_data, metric=get_sequence_distance_list)\n",
    "links = shc.linkage(clust_data, metric=get_sequence_distance_matrix)\n",
    "dend = shc.dendrogram(links, labels=stays, leaf_rotation=-90)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_fasta(sequences: list, sequence_ids, id, path=output_path, folder=output_folder):\n",
    "    if not os.path.isdir(path + folder):\n",
    "        os.makedirs(path + folder)\n",
    "\n",
    "    file_name = f\"{path + folder}sequences-{id}.fa\"\n",
    "\n",
    "    if not os.path.exists(file_name):\n",
    "        file = open(file_name, 'w')\n",
    "        for i in range(len(sequences)):\n",
    "            file.write(f\">{sequence_ids[i]}\\n{sequences[i]}\\n\")\n",
    "        file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for stay in stays:\n",
    "    events = data[data['hadm_id'] == stay]\n",
    "    # print(f\"seq {stay}: {''.join(list(events['event_encoded_alphabet']))}\")\n",
    "    sequences.append(''.join(list(events['event_encoded'])))\n",
    "    # print(f\"seq {stay}: {''.join(list(events['event_encoded']))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stay in stays:\n",
    "    print(stay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_indexes(list_data, indexes, reverse=False):\n",
    "  return [val for (_, val) in sorted(zip(indexes, list_data), key=lambda x:\n",
    "          x[0], reverse=reverse)]\n",
    "\n",
    "\n",
    "def get_clusters_by_level(level, links):\n",
    "    return list(shc.fcluster(links, t=level, criterion=\"distance\"))\n",
    "\n",
    "\n",
    "def get_aggregated_sequence(al_seq):\n",
    "    agg_sequence = list(\n",
    "        zip(*[sequence.seq for sequence in al_seq]))\n",
    "    # Remove duplicates\n",
    "    agg_sequence = [list(set(agg_event)) for agg_event in agg_sequence]\n",
    "    # Convert characters to numbers\n",
    "    agg_sequence = [[event for event in agg_event]\n",
    "                    for agg_event in agg_sequence]\n",
    "    # agg_sequence = [[str(character_to_number(event))\n",
    "    #                  for event in agg_event] for agg_event in agg_sequence]\n",
    "    # Only have lists when aggregate event\n",
    "    agg_sequence = [event[0] if len(\n",
    "        event) == 1 else event for event in agg_sequence]\n",
    "\n",
    "    return agg_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = [dend['ivl'].index(i) for i in stays]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_events(level, stays, links):\n",
    "    clusters = get_clusters_by_level(level, links)\n",
    "    unique_levels = list(set(clusters))\n",
    "    print(f\"clust: {clusters}, ul: {unique_levels}\")\n",
    "\n",
    "    cluster_level = copy.deepcopy(level)\n",
    "\n",
    "    for count, level in enumerate(unique_levels):\n",
    "        cluster = [i for i, x in enumerate(clusters) if x == level]\n",
    "        print(f\"clust: {cluster}\")\n",
    "        bidx = branch_depths.index(cluster_level)\n",
    "\n",
    "        sequence_ids = [int(stays[i]) for i in cluster]\n",
    "        print(f\"sids: {sequence_ids}\")\n",
    "\n",
    "        alignment_levels = []\n",
    "        if (cluster_level > 0):\n",
    "            alignment_levels = list(dict.fromkeys([\n",
    "                sequence_alignments[bidx - 1][stays.index(s)] for s in sequence_ids]))\n",
    "\n",
    "        if len(sequence_ids) == 1:\n",
    "            # base case: sequence need not to be merged\n",
    "            # print(\"[INFO] base case, no alignment\")\n",
    "            pass\n",
    "\n",
    "        elif (len(alignment_levels) == 1 and alignment_levels[0] == -1) or (len(alignment_levels) == 0 and cluster_level == 0):\n",
    "            # case sequences need to be merged, no prior alignment\n",
    "            print(\"[INFO] sequences need to be aligned, no alignment present\")\n",
    "\n",
    "            sequence_to_fasta(sequences=[sequences[stays.index(s)] for s in sequence_ids],\n",
    "                              sequence_ids=sequence_ids, id=f\"{cluster_level}-{count}\")\n",
    "\n",
    "            sequences_file_path = f\"{output_path + output_folder}sequences-{cluster_level}-{count}.fa\"\n",
    "            base_alignment_file_path = f\"{output_path + output_folder}alignment-{cluster_level}-{count}.fasta\"\n",
    "\n",
    "            mafft_align = f\"/usr/local/bin/mafft --text --reorder --maxiterate 0 --retree 1 --6merpair --quiet --thread 4 {sequences_file_path} > {base_alignment_file_path}\"\n",
    "            !$mafft_align\n",
    "            # os.system(mafft_align)\n",
    "\n",
    "            alignment = AlignIO.read(base_alignment_file_path, \"fasta\")\n",
    "            aggregated_sequence = get_aggregated_sequence(alignment)\n",
    "\n",
    "            cluster_alignment = {\n",
    "                'file': base_alignment_file_path,\n",
    "                'stays': sequence_ids,\n",
    "                'sequence': aggregated_sequence,\n",
    "                'alignment': [{'hadm_id': aligned_seq.id, \"sequence\": [\n",
    "                    event for event in aligned_seq.seq]} for aligned_seq in alignment]\n",
    "            }\n",
    "\n",
    "            save_as_pickle(\n",
    "                cluster_alignment, f\"alignment-info-{number_of_stays}-level-{cluster_level}-count-{count}.p\", path=output_path + alignments_output)\n",
    "\n",
    "            for s in sequence_ids:\n",
    "                sidx = stays.index(s)\n",
    "                sequence_alignments[bidx][sidx] = f\"{cluster_level}-{count}\"\n",
    "\n",
    "        elif len(alignment_levels) == 1 and alignment_levels[0] != -1:\n",
    "            # case sequences have already been merged, no action needed\n",
    "            print(\"[INFO] have been merged, no action needed\")\n",
    "            for s in sequence_ids:\n",
    "                sidx = stays.index(s)\n",
    "                sequence_alignments[bidx][sidx] = sequence_alignments[bidx - 1][sidx]\n",
    "\n",
    "        else:\n",
    "            # merging and or alignment needs to happen\n",
    "            # print(\"[INFO] merge needed and optional alignment\")\n",
    "\n",
    "            not_aligned_sequences = [\n",
    "                s for s in sequence_ids if sequence_alignments[bidx - 1][stays.index(s)] == -1]\n",
    "\n",
    "            sequence_to_fasta(sequences=[sequences[stays.index(s)] for s in not_aligned_sequences],\n",
    "                              sequence_ids=not_aligned_sequences, id=f\"{cluster_level}-{count}\")\n",
    "\n",
    "            sequences_file_path = f\"{output_path + output_folder}sequences-{cluster_level}-{count}.fa\"\n",
    "            base_alignment_file_path = f\"{output_path + output_folder}alignment-{cluster_level}-{count}.fasta\"\n",
    "\n",
    "            # Get alignment files of previous merged\n",
    "            aligned_sequences = [\n",
    "                s for s in sequence_ids if sequence_alignments[bidx - 1][stays.index(s)] != -1]\n",
    "\n",
    "            print(\n",
    "                f\"[INFO] merge needed ({len(aligned_sequences)}) and optional alignment ({len(not_aligned_sequences)})\")\n",
    "\n",
    "            aligned_files = []\n",
    "\n",
    "            for a in aligned_sequences:\n",
    "                file_details = sequence_alignments[bidx -\n",
    "                                                   1][stays.index(a)].split('-')\n",
    "                aligned_files.append(\n",
    "                    f\"{output_path + output_folder}alignment-{file_details[0]}-{file_details[1]}.fasta\")\n",
    "\n",
    "            aligned_files = list(dict.fromkeys(\n",
    "                aligned_files))  # Remove duplicates\n",
    "            table_files = \" \".join(aligned_files)\n",
    "            aligned_files.append(sequences_file_path)\n",
    "            input_files = \" \".join(aligned_files)\n",
    "\n",
    "            # Create merge table for MAFFT\n",
    "            merge_table = f\"/usr/bin/ruby makemergetable.rb {table_files} > subMSAtable\"\n",
    "            # os.system(merge_table)\n",
    "            !$merge_table\n",
    "\n",
    "            # Create input file\n",
    "            input_command = f\"cat {input_files} > inputFile\"\n",
    "            !$input_command\n",
    "\n",
    "            # os.system(input_command)\n",
    "\n",
    "            mafft_merge = f\"/usr/local/bin/mafft --merge subMSAtable --text --reorder --maxiterate 0 --retree 1 --6merpair --quiet --thread 4 inputFile > {base_alignment_file_path}\"\n",
    "            # os.system(mafft_merge)\n",
    "            !$mafft_merge\n",
    "\n",
    "            alignment = AlignIO.read(base_alignment_file_path, \"fasta\")\n",
    "            aggregated_sequence = get_aggregated_sequence(alignment)\n",
    "\n",
    "            cluster_alignment = {\n",
    "                'file': base_alignment_file_path,\n",
    "                'stays': sequence_ids,\n",
    "                'sequence': aggregated_sequence,\n",
    "                'alignment': [{'hadm_id': aligned_seq.id, \"sequence\": [\n",
    "                    event for event in aligned_seq.seq]} for aligned_seq in alignment]\n",
    "            }\n",
    "\n",
    "            save_as_pickle(\n",
    "                cluster_alignment, f\"alignment-info-{number_of_stays}-level-{cluster_level}-count-{count}.p\", path=output_path + alignments_output)\n",
    "\n",
    "            for s in sequence_ids:\n",
    "                sidx = stays.index(s)\n",
    "                sequence_alignments[bidx][sidx] = f\"{cluster_level}-{count}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_depths = [-1]\n",
    "for d in dend['dcoord']:\n",
    "    branch_depths.append(d[1])\n",
    "branch_depths = list(dict.fromkeys(branch_depths))\n",
    "branch_depths.sort()\n",
    "\n",
    "if not os.path.isdir(output_path + alignments_output):\n",
    "    os.makedirs(output_path + alignments_output)\n",
    "\n",
    "sequence_alignments = [[-1] * len(stays) for i in range(len(branch_depths))]\n",
    "\n",
    "for index, branch_depth in enumerate(tqdm(branch_depths)):\n",
    "    print(f\"Aligning level {index, branch_depth}\")\n",
    "    cluster_events(branch_depth, stays, links)\n",
    "\n",
    "# save_as_pickle(sequence_alignments, 'alignments_' + str(number_of_stays))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "transfer_label_data = data[['eventtype', 'careunit', 'event_encoded']]\n",
    "icu_label_data = data[['label', 'value',\n",
    "                       'valuenum',\t'valueuom', 'event_encoded']]\n",
    "\n",
    "for event_type in tqdm(transfer_label_data['eventtype'].unique()):\n",
    "    # values = data[data['eventtype'] == event_type].drop_duplicates()\n",
    "    values = transfer_label_data[transfer_label_data['eventtype']\n",
    "                                 == event_type].drop_duplicates()\n",
    "    if len(values) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        label = {\n",
    "            'type': 'Transfer',\n",
    "            'care_unit': event_type,\n",
    "            'value_enc': values.iloc[0, 2],\n",
    "            'values': [{'event_type': v.careunit} for v in values.itertuples(index=True)]\n",
    "        }\n",
    "\n",
    "    labels.append(label)\n",
    "\n",
    "for label in tqdm(icu_label_data['label'].unique()):\n",
    "    values = icu_label_data[icu_label_data['label'] == label].drop_duplicates()\n",
    "    if len(values) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        label = {\n",
    "            'type': 'ICU measurement',\n",
    "            'measurement': label,\n",
    "            'value_enc': values.iloc[0, 4],\n",
    "            'unit': values.iloc[0, 3] if not pd.isnull(values.iloc[0, 3]) else -1,\n",
    "            'values': [{'value': v.value} for v in values.itertuples(index=True)]\n",
    "        }\n",
    "    labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_pickle(links, 'links' + file_suffix)\n",
    "save_as_pickle(dist_matrix, 'dist_matrix' + file_suffix) \n",
    "save_as_pickle(stays, 'stays' + file_suffix)\n",
    "save_as_pickle(sequence_alignments, 'alignments_' + str(number_of_stays))\n",
    "save_as_pickle(labels, 'labels' + file_suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_alignments[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend-F9edf9D0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8aa693308f7ebf26aebd877d93b16497504fa2934292b1fef41da166959c0225"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
