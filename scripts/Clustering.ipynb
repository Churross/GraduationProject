{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Clustering, Alignment and labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import data and libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# import sys\n",
    "import os\n",
    "# import scipy as sc\n",
    "# from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import datetime as dt\n",
    "import math\n",
    "import pickle\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from tqdm import tqdm\n",
    "# import time\n",
    "from Bio import AlignIO\n",
    "import copy\n",
    "# from spmf import Spmf\n",
    "# from Bio.Align.Applications import MafftCommandline as mafft\n",
    "from levenstein_dp import levenshteinDistanceDP as ld\n",
    "import editdistance\n",
    "\n",
    "\n",
    "# sys.setrecursionlimit(5000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# File export suffix\n",
    "start_index = 0\n",
    "number_of_stays = 1000\n",
    "display_matrix = False\n",
    "output_path = \"output/\"\n",
    "run_test_data = True\n",
    "output_folder = f\"stays-{number_of_stays}/\"\n",
    "alignments_output = f\"{output_folder}alignments/\"\n",
    "number_of_events_test = 20\n",
    "\n",
    "# output_folder = f\"stays-test-{number_of_stays}/\"\n",
    "\n",
    "# file_suffix = '_test_' + str(number_of_stays)\n",
    "file_suffix = '_' + str(number_of_stays)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Store and load function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_as_pickle(data, file_name, path=output_path):\n",
    "    file = open(path + file_name, 'wb')\n",
    "    pickle.dump(data, file)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "def get_pickle(file_name, path=output_path):\n",
    "    return pickle.load(open(path + file_name, 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Store as fasta file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sequence_to_fasta(sequences: list, sequence_ids, id, path=output_path, folder=output_folder):\n",
    "    if not os.path.isdir(path + folder):\n",
    "        print('[INFO] creating folder')\n",
    "        os.makedirs(path + folder)\n",
    "        \n",
    "    file = open(f\"{path + folder}sequences-{id}.fa\", 'w')\n",
    "   \n",
    "    print('[INFO] writing file...')\n",
    "    for i in range(len(sequences)):\n",
    "        file.write(f\">{sequence_ids[i]}\\n{sequences[i]}\\n\")\n",
    "    file.close()\n",
    "    print('[INFO] writing done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load complete dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = get_pickle('data_complete_v4.1')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load distance matrix dataset & set stays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dist_data = get_pickle(\"distance_data_v4.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist_data.sort_values(by='event_id')\n",
    "# dist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stays = list(dist_data['hadm_id'].unique())[\n",
    "    start_index: start_index + number_of_stays]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test dataset of limited length and \\# stays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_test_data:\n",
    "    test_data = pd.DataFrame(columns=data.columns)\n",
    "    test_data_dist = pd.DataFrame(columns=dist_data.columns)\n",
    "    for stay in stays:\n",
    "        events = data[data['hadm_id'] == stay]\n",
    "        events_dist = dist_data[dist_data['hadm_id'] == stay]\n",
    "        test_data = pd.concat(\n",
    "            [test_data, events.head(n=number_of_events_test)])\n",
    "        test_data_dist = pd.concat(\n",
    "            [test_data_dist, events_dist.head(n=number_of_events_test)])\n",
    "\n",
    "    test_data = test_data.reset_index()\n",
    "    test_data_dist = test_data_dist.reset_index()\n",
    "    test_data = test_data.drop(columns=['index'])\n",
    "    test_data_dist = test_data_dist.drop(columns=['index'])\n",
    "    test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_test_data:\n",
    "    data = test_data\n",
    "    dist_data = test_data_dist\n",
    "    save_as_pickle(data, 'events' + file_suffix + \"_test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Compute distance matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_pickle(data, 'events' + file_suffix + \"_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def compute_distance_matrix(number_of_stays=number_of_stays):\n",
    "#     stays = list(dist_data['hadm_id'].unique())[\n",
    "#         start_index: start_index + number_of_stays]\n",
    "#     distances = []\n",
    "\n",
    "#     for y in tqdm(range(len(stays))):\n",
    "#         sequence_y = dist_data[dist_data['hadm_id']\n",
    "#                                == stays[y]]['event_encoded'].tolist()\n",
    "#         distance_row = []\n",
    "\n",
    "#         for x in tqdm(range(y, len(stays))):\n",
    "#             sequence_x = dist_data[dist_data['hadm_id']\n",
    "#                                    == stays[x]]['event_encoded'].tolist()\n",
    "\n",
    "#             # Diagonal\n",
    "#             if stays[y] == stays[x]:\n",
    "#                 distance_row.append(0)\n",
    "#                 continue\n",
    "#             # All other\n",
    "#             else:\n",
    "#                 max_length = max(len(sequence_x), len(sequence_y))\n",
    "#                 distance_row.append(ld(sequence_y, sequence_x)/max_length)\n",
    "\n",
    "#         distances.append(distance_row)\n",
    "\n",
    "#     # print(distances)\n",
    "#     if display_matrix:\n",
    "#         print('Computed distance matrix:')\n",
    "\n",
    "#         for line in distances:\n",
    "#             print('  '.join(map(str, line)))\n",
    "#     return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix():\n",
    "    sequences = [dist_data[dist_data['hadm_id']\n",
    "                           == hadm_id]['event_encoded'].tolist() for hadm_id in stays]\n",
    "\n",
    "    print(\"[INFO] Data Loaded \")\n",
    "\n",
    "    length = len(sequences)\n",
    "    outputMatrix = [[0] * length for _i in range(length)]\n",
    "\n",
    "    progress = 0\n",
    "    updateStep = 100\n",
    "    with tqdm(total=0.5*(length * length)) as pbar:\n",
    "        for idxA in range(0, length):\n",
    "            for idxB in range(idxA, length):\n",
    "                max_length = max(len(sequences[idxA]), len(sequences[idxB]))\n",
    "                distance = editdistance.eval(\n",
    "                    sequences[idxA], sequences[idxB])/max_length\n",
    "                outputMatrix[idxA][idxB] = distance\n",
    "                outputMatrix[idxB][idxA] = distance\n",
    "                if (progress % updateStep == 0):\n",
    "                    pbar.update(updateStep)\n",
    "                progress += 1\n",
    "    \n",
    "    return outputMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dist_matrix = compute_distance_matrix()\n",
    "# save_as_pickle(dist_matrix, 'distance_matrix_test_' + str(number_of_stays))\n",
    "save_as_pickle(dist_matrix, 'distance_matrix_' + str(number_of_stays))\n",
    "save_as_pickle(dist_matrix, 'dist_matrix_' + str(number_of_stays))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dist_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Hierarchical clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dist_matrix = get_pickle('distance_matrix_50')\n",
    "# dist_matrix = get_pickle('distance_matrix_3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = get_pickle('distance_matrix_' + str(number_of_stays))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def get_sequence_distance_list(u, v):\n",
    "#     index_u, index_v = stays.index(u[0]), stays.index(v[0])\n",
    "#     return dist_matrix[min(index_u, index_v)][max(index_u, index_v) - min(index_u, index_v)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_distance_matrix(u, v):\n",
    "    index_u, index_v = stays.index(u[0]), stays.index(v[0])\n",
    "    return dist_matrix[index_u][index_v]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clust_data = data.drop_duplicates(subset=['hadm_id'])[\n",
    "    start_index: start_index + number_of_stays]\n",
    "\n",
    "clust_data = clust_data.drop(columns=['event_id', 'subject_id', 'transfer_id', 'eventtype',\n",
    "                                      'careunit', 'intime', 'outtime', 'charttime', 'event',\n",
    "                                      'value', 'valuenum', 'valueuom',\n",
    "                                      'label', 'category', 'param_type',\n",
    "                                      'value_categorical',\n",
    "                                      'event_encoded'])\n",
    "\n",
    "# links = shc.linkage(clust_data, metric=get_sequence_distance_list)\n",
    "links = shc.linkage(clust_data, metric=get_sequence_distance_matrix)\n",
    "dend = shc.dendrogram(links, labels=stays, leaf_rotation=-90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display test sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for stay in stays:\n",
    "    events = data[data['hadm_id'] == stay]\n",
    "    # print(f\"seq {stay}: {''.join(list(events['event_encoded_alphabet']))}\")\n",
    "    sequences.append(''.join(list(events['event_encoded'])))\n",
    "    # print(f\"seq {stay}: {''.join(list(events['event_encoded']))}\")\n",
    "\n",
    "# sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Aligning sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_indexes(list_data, indexes, reverse=False):\n",
    "  return [val for (_, val) in sorted(zip(indexes, list_data), key=lambda x:\n",
    "          x[0], reverse=reverse)]\n",
    "  \n",
    "def get_clusters_by_level(level, links):\n",
    "    return list(shc.fcluster(links, t=level, criterion=\"distance\"))\n",
    "\n",
    "\n",
    "def get_aggregated_sequence(al_seq):\n",
    "    agg_sequence = list(\n",
    "        zip(*[sequence.seq for sequence in al_seq]))\n",
    "    # Remove duplicates\n",
    "    agg_sequence = [list(set(agg_event)) for agg_event in agg_sequence]\n",
    "    # Convert characters to numbers\n",
    "    agg_sequence = [[event for event in agg_event]\n",
    "                    for agg_event in agg_sequence]\n",
    "    # agg_sequence = [[str(character_to_number(event))\n",
    "    #                  for event in agg_event] for agg_event in agg_sequence]\n",
    "    # Only have lists when aggregate event\n",
    "    agg_sequence = [event[0] if len(\n",
    "        event) == 1 else event for event in agg_sequence]\n",
    "\n",
    "    return agg_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stays_old = copy.deepcopy(stays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [dend['ivl'].index(i) for i in stays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_clusters_by_level(0.6, links)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_events(level, stays, links):\n",
    "    clusters = get_clusters_by_level(level, links)\n",
    "    unique_levels = list(set(clusters))\n",
    "    print(f\"clust: {clusters}, ul: {unique_levels}\")\n",
    "   \n",
    "    cluster_level = copy.deepcopy(level)\n",
    "\n",
    "    for count, level in enumerate(unique_levels):\n",
    "        cluster = [i for i, x in enumerate(clusters) if x == level]\n",
    "        print(f\"clust: {cluster}\")\n",
    "        bidx = branch_depths.index(cluster_level)\n",
    "\n",
    "        sequence_ids = [int(stays[i]) for i in cluster]\n",
    "        print(f\"sids: {sequence_ids}\")\n",
    "       \n",
    "        alignment_levels = []\n",
    "        if (cluster_level > 0):\n",
    "            alignment_levels = list(dict.fromkeys([\n",
    "                sequence_alignments[bidx - 1][stays.index(s)] for s in sequence_ids]))\n",
    "\n",
    "\n",
    "        if len(sequence_ids) == 1:\n",
    "            # base case: sequence need not to be merged\n",
    "            # print(\"[INFO] base case, no alignment\")\n",
    "            pass\n",
    "\n",
    "        elif (len(alignment_levels) == 1 and alignment_levels[0] == -1) or (len(alignment_levels) == 0 and cluster_level == 0):\n",
    "            # case sequences need to be merged, no prior alignment\n",
    "            print(\"[INFO] sequences need to be aligned, no alignment present\")\n",
    "\n",
    "            \n",
    "            sequence_to_fasta(sequences=[sequences[stays.index(s)] for s in sequence_ids],\n",
    "                              sequence_ids=sequence_ids, id=f\"{cluster_level}-{count}\")\n",
    "\n",
    "            sequences_file_path = f\"{output_path + output_folder}sequences-{cluster_level}-{count}.fa\"\n",
    "            base_alignment_file_path = f\"{output_path + output_folder}alignment-{cluster_level}-{count}.fasta\"\n",
    "\n",
    "            mafft_align = f\"/usr/local/bin/mafft --reorder --anysymbol --maxiterate 0 --retree 1 --6merpair --quiet --thread 4 {sequences_file_path} > {base_alignment_file_path}\"\n",
    "            !$mafft_align\n",
    "            # os.system(mafft_align)\n",
    "\n",
    "            alignment = AlignIO.read(base_alignment_file_path, \"fasta\")\n",
    "            aggregated_sequence = get_aggregated_sequence(alignment)\n",
    "\n",
    "            cluster_alignment = {\n",
    "                'file': base_alignment_file_path,\n",
    "                'stays': sequence_ids,\n",
    "                'sequence': aggregated_sequence,\n",
    "                'alignment': [{'hadm_id': aligned_seq.id, \"sequence\": [\n",
    "                    event for event in aligned_seq.seq]} for aligned_seq in alignment]\n",
    "            }\n",
    "\n",
    "            save_as_pickle(\n",
    "                cluster_alignment, f\"alignment-info-{number_of_stays}-level-{cluster_level}-count-{count}.p\", path=output_path + alignments_output)\n",
    "\n",
    "            for s in sequence_ids:\n",
    "                sidx = stays.index(s)\n",
    "                sequence_alignments[bidx][sidx] = f\"{cluster_level}-{count}\"\n",
    "\n",
    "        elif len(alignment_levels) == 1 and alignment_levels[0] != -1:\n",
    "            # case sequences have already been merged, no action needed\n",
    "            print(\"[INFO] have been merged, no action needed\")\n",
    "            for s in sequence_ids:\n",
    "                sidx = stays.index(s)\n",
    "                sequence_alignments[bidx][sidx] = sequence_alignments[bidx - 1][sidx]\n",
    "\n",
    "        else:\n",
    "            # merging and or alignment needs to happen\n",
    "            # print(\"[INFO] merge needed and optional alignment\")\n",
    "\n",
    "            not_aligned_sequences = [\n",
    "                s for s in sequence_ids if sequence_alignments[bidx - 1][stays.index(s)] == -1]\n",
    "            \n",
    "            sequence_to_fasta(sequences=[sequences[stays.index(s)] for s in not_aligned_sequences],\n",
    "                              sequence_ids=not_aligned_sequences, id=f\"{cluster_level}-{count}\")\n",
    "\n",
    "            sequences_file_path = f\"{output_path + output_folder}sequences-{cluster_level}-{count}.fa\"\n",
    "            base_alignment_file_path = f\"{output_path + output_folder}alignment-{cluster_level}-{count}.fasta\"\n",
    "\n",
    "            # Get alignment files of previous merged\n",
    "            aligned_sequences = [\n",
    "                s for s in sequence_ids if sequence_alignments[bidx - 1][stays.index(s)] != -1]\n",
    "\n",
    "            print(\n",
    "                f\"[INFO] merge needed ({len(aligned_sequences)}) and optional alignment ({len(not_aligned_sequences)})\")\n",
    "\n",
    "            aligned_files = []\n",
    "\n",
    "            for a in aligned_sequences:\n",
    "                file_details = sequence_alignments[bidx -\n",
    "                                                   1][stays.index(a)].split('-')\n",
    "                aligned_files.append(\n",
    "                    f\"{output_path + output_folder}alignment-{file_details[0]}-{file_details[1]}.fasta\")\n",
    "\n",
    "            aligned_files = list(dict.fromkeys(\n",
    "                aligned_files))  # Remove duplicates\n",
    "            table_files = \" \".join(aligned_files)\n",
    "            aligned_files.append(sequences_file_path)\n",
    "            input_files = \" \".join(aligned_files)\n",
    "\n",
    "            # Create merge table for MAFFT\n",
    "            merge_table = f\"/usr/bin/ruby makemergetable.rb {table_files} > subMSAtable\"\n",
    "            # os.system(merge_table)\n",
    "            !$merge_table\n",
    "\n",
    "            # Create input file\n",
    "            input_command = f\"cat {input_files} > inputFile\"\n",
    "            !$input_command\n",
    "\n",
    "            # os.system(input_command)\n",
    "\n",
    "            mafft_merge = f\"/usr/local/bin/mafft --merge subMSAtable --reorder --anysymbol --maxiterate 0 --retree 1 --6merpair --quiet --thread 4 inputFile > {base_alignment_file_path}\"\n",
    "            # os.system(mafft_merge)\n",
    "            !$mafft_merge\n",
    "\n",
    "            alignment = AlignIO.read(base_alignment_file_path, \"fasta\")\n",
    "            aggregated_sequence = get_aggregated_sequence(alignment)\n",
    "\n",
    "            cluster_alignment = {\n",
    "                'file': base_alignment_file_path,\n",
    "                'stays': sequence_ids,\n",
    "                'sequence': aggregated_sequence,\n",
    "                'alignment': [{'hadm_id': aligned_seq.id, \"sequence\": [\n",
    "                    event for event in aligned_seq.seq]} for aligned_seq in alignment]\n",
    "            }\n",
    "\n",
    "            save_as_pickle(\n",
    "                cluster_alignment, f\"alignment-info-{number_of_stays}-level-{cluster_level}-count-{count}.p\", path=output_path + alignments_output)\n",
    "\n",
    "            for s in sequence_ids:\n",
    "                sidx = stays.index(s)\n",
    "                sequence_alignments[bidx][sidx] = f\"{cluster_level}-{count}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "branch_depths = [-1]\n",
    "for d in dend['dcoord']:\n",
    "    branch_depths.append(d[1])\n",
    "branch_depths = list(dict.fromkeys(branch_depths))\n",
    "branch_depths.sort()\n",
    "\n",
    "if not os.path.isdir(output_path + alignments_output):\n",
    "    os.makedirs(output_path + alignments_output)\n",
    "\n",
    "sequence_alignments = [[-1] * len(stays) for i in range(len(branch_depths))]\n",
    "\n",
    "for index, branch_depth in enumerate(tqdm(branch_depths)):\n",
    "    print(f\"Aligning level {index, branch_depth}\")\n",
    "    cluster_events(branch_depth, stays, links )\n",
    "\n",
    "# save_as_pickle(sequence_alignments, 'alignments_' + str(number_of_stays))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create labels for frontend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "careunits = data[data['eventtype'] ==\n",
    "                 data['eventtype'].unique()[2]]\n",
    "# values = data[(data['eventtype'] == data['eventtype'].unique()[2])\n",
    "#               & data['careunit'].isin(careunits)]\n",
    "\n",
    "\n",
    "label_data = data[['eventtype', 'careunit', 'event_encoded']]\n",
    "label_data['eventtype'].unique()\n",
    "# values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "icu_label_data = data[['label', 'value',\n",
    "                       'valuenum',\t'valueuom', 'event_encoded']]\n",
    "icu_label_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "transfer_label_data = data[['eventtype', 'careunit', 'event_encoded']]\n",
    "icu_label_data = data[['label', 'value', 'valuenum', 'valueuom']]\n",
    "\n",
    "\n",
    "for event_type in tqdm(transfer_label_data['eventtype'].unique()):\n",
    "    # values = data[data['eventtype'] == event_type].drop_duplicates()\n",
    "    values = transfer_label_data[transfer_label_data['eventtype']\n",
    "                                 == event_type].drop_duplicates()\n",
    "    if len(values) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        label = {\n",
    "            'type': 'Transfer',\n",
    "            'care_unit': event_type,\n",
    "            'value_enc': values.iloc[0, 2],\n",
    "            'values': [{'event_type': v.careunit} for v in values.itertuples(index=True)]\n",
    "        }\n",
    "\n",
    "    labels.append(label)\n",
    "\n",
    "for label in tqdm(icu_label_data['label'].unique()):\n",
    "    values = icu_label_data[icu_label_data['label'] == label].drop_duplicates()\n",
    "    if len(values) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        label = {\n",
    "            'type': 'ICU measurement',\n",
    "            'measurement': label,\n",
    "            'value_enc': values.iloc[0, 3],\n",
    "            'values': [{'value': v.value, 'value_unit': v.valueuom if not pd.isnull(v.valueuom) else -1} for v in values.itertuples(index=True)]\n",
    "        }\n",
    "    labels.append(label)\n",
    "\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# labels = []\n",
    "\n",
    "# total_unique_labels = []\n",
    "\n",
    "# # total_unique_labels.append(data['label'].unique())\n",
    "# # total_unique_labels.append(data['careunit'].unique())\n",
    "# # total_unique_labels.append(data['PerformedProcedureStepDescription'].unique());\n",
    "\n",
    "\n",
    "# for care_unit in tqdm(data['careunit'].unique()):\n",
    "#     values = data[data['careunit'] == care_unit].drop_duplicates()\n",
    "#     label = {\n",
    "#         'type': 'Transfer',\n",
    "#         'care_unit': care_unit,\n",
    "#         'values': [{'event_type': v.eventtype, 'value_enc': v.value_categorical} for v in values.itertuples(index=True)]\n",
    "#     }\n",
    "#     labels.append(label)\n",
    "\n",
    "# # for photo_description in tqdm(data['ProcedureCodeSequence_CodeMeaning'].unique()):\n",
    "# #     values = data[data['ProcedureCodeSequence_CodeMeaning']\n",
    "# #                   == photo_description]\n",
    "# #     label = {\n",
    "# #         'type': 'Photo',\n",
    "# #         'viewpoint': photo_description,\n",
    "# #         'values': [{'view_position': v.ViewPosition, 'patient_orientation': v.PatientOrientationCodeSequence_CodeMeaning, 'value_enc': v.value_categorical} for v in values.itertuples(index=True)]\n",
    "# #     }\n",
    "# #     labels.append(label)\n",
    "\n",
    "# for label in tqdm(data['label'].unique()):\n",
    "#     values = data[data['label'] == label].drop_duplicates()\n",
    "#     label = {\n",
    "#         'type': 'ICU measurement',\n",
    "#         'measurement': label,\n",
    "#         'values': [{'value': v.value, 'value_unit': v.valueuom if not pd.isnull(v.valueuom) else -1, 'value_enc': v.value_categorical, } for v in values.itertuples(index=True)]\n",
    "#     }\n",
    "#     labels.append(label)\n",
    "\n",
    "\n",
    "# labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "changed = False\n",
    "\n",
    "if 'care_unit' in labels[0] and (type(labels[0]['care_unit']) == 'int' and math.isnan(labels[0]['care_unit'])):\n",
    "    labels[0]['care_unit'] = 'Not available'\n",
    "    print(labels[0])\n",
    "    changed = True\n",
    "\n",
    "if 'measurement' in labels[len(labels) - 1] and type(labels[len(labels) - 1]) == 'int' and math.isnan(labels[len(labels) - 1]['measurement']):\n",
    "    labels[len(labels) - 1]['measurement'] = \"Not available\"\n",
    "    print(labels[len(labels) - 1])\n",
    "    changed = True\n",
    "\n",
    "\n",
    "if changed:\n",
    "    print('saving...')\n",
    "    save_as_pickle(labels, 'labels')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Store data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_as_pickle(links, 'links' + file_suffix)\n",
    "# save_as_pickle(dist_matrix, 'dist_matrix' +\n",
    "#                file_suffix)  # todo: check if correct\n",
    "save_as_pickle(stays, 'stays' + file_suffix)\n",
    "# save_as_pickle(data[data['hadm_id'].isin(stays)], 'events' + file_suffix)\n",
    "save_as_pickle(sequence_alignments, 'alignments_' + str(number_of_stays))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_details(hadm_id, event_index):\n",
    "    event = data[data['hadm_id'] == int(\n",
    "        hadm_id)].iloc[[event_index]]\n",
    "\n",
    "    event = event[event.columns[~event.isnull().all()]]\n",
    "\n",
    "    return event\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8aa693308f7ebf26aebd877d93b16497504fa2934292b1fef41da166959c0225"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
